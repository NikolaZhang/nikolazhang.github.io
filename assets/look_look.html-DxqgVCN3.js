import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,e as i,o as l}from"./app-DgU4tkOd.js";const n={};function r(o,t){return l(),a("div",null,[...t[0]||(t[0]=[i(`<blockquote><p>基于scrapy框架的爬虫项目，用于爬取常见网站的信息。<br> 不要遮遮掩掩的，让我康康！</p></blockquote><h2 id="环境配置" tabindex="-1"><a class="header-anchor" href="#环境配置"><span>环境配置</span></a></h2><h3 id="创建conda环境" tabindex="-1"><a class="header-anchor" href="#创建conda环境"><span>创建conda环境</span></a></h3><p>conda create -n look_look python=3.11.2</p><h3 id="安装依赖" tabindex="-1"><a class="header-anchor" href="#安装依赖"><span>安装依赖</span></a></h3><p>conda install --yes --file requirements.txt</p><h2 id="目录结构" tabindex="-1"><a class="header-anchor" href="#目录结构"><span>目录结构</span></a></h2><h3 id="python包" tabindex="-1"><a class="header-anchor" href="#python包"><span>python包</span></a></h3><ol><li><p>config<br> 自定义的系统配置，比如驱动，文件路径，redis key。<br> 该包下的配置，可以在项目中直接引用，或者使用scrapy框架内的spider对象获取settings。<br> 如果想让后者生效，需要在项目中的<code>settings.py</code>中引入对应的python文件，如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> config.driver_config </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> *</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> config.web_config </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> *</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> config.path_config </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> *</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>look_look/enhance<br> 通常用于定义一些接口，或者爬虫通用的实现方法。</p></li><li><p>look_look/spider<br> scrapy框架spider放到这个地方</p></li><li><p><a href="http://items.py" target="_blank" rel="noopener noreferrer">items.py</a><br> 用于定义一些页面的字段</p></li><li><p><a href="http://middlewares.py" target="_blank" rel="noopener noreferrer">middlewares.py</a><br> 用于处理请求和响应</p></li><li><p><a href="http://pipelines.py" target="_blank" rel="noopener noreferrer">pipelines.py</a><br> 用于处理item</p></li><li><p>utils<br> 工具方法</p></li></ol><h3 id="pipelines" tabindex="-1"><a class="header-anchor" href="#pipelines"><span>pipelines</span></a></h3><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">ExcelPipeline</td><td style="text-align:left;">将item信息保存到excel，支持header自定义</td></tr><tr><td style="text-align:left;">ImageSavePipeline</td><td style="text-align:left;">图片链接保存</td></tr><tr><td style="text-align:left;">MarkdownPipeline</td><td style="text-align:left;">将html转换为markdown</td></tr><tr><td style="text-align:left;">MongoDBPipeline</td><td style="text-align:left;">将item信息存储到mongo</td></tr><tr><td style="text-align:left;">RedisPipeline</td><td style="text-align:left;">将链接存放到redis</td></tr></tbody></table><h3 id="middlewares" tabindex="-1"><a class="header-anchor" href="#middlewares"><span>middlewares</span></a></h3><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">ChromeDownloaderMiddleware</td><td style="text-align:left;">使用selenium获取动态网页内容</td></tr><tr><td style="text-align:left;">RandomUserAgentMiddleware</td><td style="text-align:left;">生成随机user-agent</td></tr><tr><td style="text-align:left;">CustomHeadersMiddleware</td><td style="text-align:left;">添加自定义的header，同时还具有缓存header的功能，不过似乎没什么用e_e。</td></tr></tbody></table><h3 id="enhance" tabindex="-1"><a class="header-anchor" href="#enhance"><span>enhance</span></a></h3><ol><li><p>base_item</p><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">IItemSaveAction</td><td style="text-align:left;">提供保存item信息时需要提供的字段，下面的Action为具体化后的接口，配合指定的pipeline使用</td></tr><tr><td style="text-align:left;">RedisItemSaveAction</td><td style="text-align:left;">配合RedisPipeline使用</td></tr><tr><td style="text-align:left;">ExcelItemSaveAction</td><td style="text-align:left;">配合ExcelPipeline使用</td></tr><tr><td style="text-align:left;">MarkdownItemSaveAction</td><td style="text-align:left;">配合MarkdownPipeline使用</td></tr><tr><td style="text-align:left;">ImageItemSaveAction</td><td style="text-align:left;">配合ImageSavePipeline使用</td></tr></tbody></table></li><li><p>base_spider</p><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">method</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">BaseSpider</td><td style="text-align:left;">get_run_config</td><td style="text-align:left;">加载运行配置:spider_config.RUN_SPIDER</td></tr><tr><td style="text-align:left;">RedisCategorySpider</td><td style="text-align:left;">get_category</td><td style="text-align:left;">存放redis时，指定key</td></tr><tr><td style="text-align:left;">RedisCategorySpider</td><td style="text-align:left;">get_key</td><td style="text-align:left;">存放redis时，生成key</td></tr><tr><td style="text-align:left;">SeleniumSpider</td><td style="text-align:left;">middleware_apply</td><td style="text-align:left;">使用selenium进行爬取</td></tr><tr><td style="text-align:left;">CustomHeaderSpider</td><td style="text-align:left;">add</td><td style="text-align:left;">添加header，比如下载pixiv图片需要referer</td></tr><tr><td style="text-align:left;">CustomHeaderSpider</td><td style="text-align:left;">get_custom_headers</td><td style="text-align:left;">获取自定义的header</td></tr><tr><td style="text-align:left;">BaseLoginSpider</td><td style="text-align:left;">login</td><td style="text-align:left;">提供登录接口，所有的登录都要实现该接口</td></tr><tr><td style="text-align:left;">PixivLoginSpider</td><td style="text-align:left;">login</td><td style="text-align:left;">pixiv的登录实现</td></tr></tbody></table></li><li><p>item_loaders</p><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">CleanItemLoader</td><td style="text-align:left;">自定义的itemloader，用于清理item字段的空格</td></tr></tbody></table></li></ol><h3 id="其他" tabindex="-1"><a class="header-anchor" href="#其他"><span>其他</span></a></h3><ol><li>logs 日志存放目录</li><li>data 爬取的文件存放目录</li></ol><blockquote><p>如果缺少这两个目录需要创建。或者在<code>settings.py</code>、以及上文说的<code>config</code>中另外指定</p></blockquote><h2 id="运行" tabindex="-1"><a class="header-anchor" href="#运行"><span>运行</span></a></h2><p>运行项目只需要启动<code>main.py</code>即可。</p><h2 id="过程与结果" tabindex="-1"><a class="header-anchor" href="#过程与结果"><span>过程与结果</span></a></h2><p>以csdn文章爬取为例：</p><ol><li>首先通过<code>CSDNSearchSpider</code>查询相关主题的文章链接，并保存到redis中。</li><li><code>CSDNArticleSpider</code>会监听redis中特定key下面是否有文章链接。有则进行文章的爬取。</li><li>提取文章的标题、作者、点赞量、收藏量、阅读量等信息存储到excel中；并将文章的内容转换为markdown格式，保存到data目录中。</li></ol><h3 id="文章链接" tabindex="-1"><a class="header-anchor" href="#文章链接"><span>文章链接</span></a></h3><figure><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/url.png" alt="文章链接" tabindex="0" loading="lazy"><figcaption>文章链接</figcaption></figure><h3 id="文章信息" tabindex="-1"><a class="header-anchor" href="#文章信息"><span>文章信息</span></a></h3><figure><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/article.png" alt="文章信息" tabindex="0" loading="lazy"><figcaption>文章信息</figcaption></figure><h3 id="文章内容" tabindex="-1"><a class="header-anchor" href="#文章内容"><span>文章内容</span></a></h3><figure><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/article_content.png" alt="文章内容" tabindex="0" loading="lazy"><figcaption>文章内容</figcaption></figure><h3 id="如何创建一个新的网站spider" tabindex="-1"><a class="header-anchor" href="#如何创建一个新的网站spider"><span>如何创建一个新的网站spider?</span></a></h3><ol><li>首先在spiders包下，新增该网站的爬虫的py文件，名字最好是该网站的名字</li><li>接下来你需要观察网页的构造。一般情况下，分为两个层次，最外层是一个列表，每一项会跳转到对应的详情页。</li><li>通常列表页，需要创建一个spider，用于爬取这些详情页的链接，爬取到的链接需要放到redis中。</li><li>之后再创建一个spider，用于爬取详情页数据。这里的spider需要是RedisSpider的子类。并且指定redis_key。</li><li>如果我们希望添加一些pipeline的能力，需要设置spider的custom_settings属性</li><li>spider的parse方法中需要进行字段的提取，建议使用CleanItemLoader，它可以将字段首位的空格换行符号去除。</li></ol><p><strong>关于item的创建：</strong><br> 在第3步中，我们爬取链接并放到redis中，此时我们需要item具有提供url值的能力。只需要实现RedisItemSaveAction的get_category_field方法即可， 该方法需要返回字段名称，而不是字段值。</p><p>在base_item中还有其他的item父类，用于支持对应的pipeline能力。如：将item保存到mongo；导出item到excel；保存图片；保存网页为markdown等。</p><p>item一般放在items.py中，当然也可以创建新的文件存放。该项目之后都会放到spider中。（毕竟错误提交或者多提交了个人的spider相关的东西，被别人看到，不太好）</p><p>你可以参考项目中已经存在spider进行编程。</p><h2 id="调试" tabindex="-1"><a class="header-anchor" href="#调试"><span>调试</span></a></h2><p>执行<code>main.py</code>可以使用debug模式。但是使用命令行则不会触发debug。<br> 如果程序报错，可以查看logs目录下的<code>look_look.log</code>文件中的报错信息。</p><h2 id="支持的网站及功能" tabindex="-1"><a class="header-anchor" href="#支持的网站及功能"><span>支持的网站及功能</span></a></h2><h3 id="csdn" tabindex="-1"><a class="header-anchor" href="#csdn"><span>csdn</span></a></h3><ul><li>[x] 查询页博客链接提取（存储到redis）</li><li>[x] 根据博客链接爬取文章（保存excel，mongo，markdown）</li></ul><h4 id="效果" tabindex="-1"><a class="header-anchor" href="#效果"><span>效果</span></a></h4><p>见上【过程与结果】</p><h3 id="pixiv" tabindex="-1"><a class="header-anchor" href="#pixiv"><span>pixiv</span></a></h3><ul><li>[x] 获取用户下的作品集</li><li>[x] 作品及详情信息获取</li><li>[x] 图片下载</li></ul><h4 id="效果展示" tabindex="-1"><a class="header-anchor" href="#效果展示"><span>效果展示</span></a></h4><p>mongodb中存储信息<br><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work1.png" alt="art1" loading="lazy"> 导出到excel中的信息<br><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work2.png" alt="art1" loading="lazy"> 保存的图片<br><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work3.png" alt="art1" loading="lazy"></p><h2 id="q-a" tabindex="-1"><a class="header-anchor" href="#q-a"><span>Q&amp;A</span></a></h2><ol><li>如何指定启动哪些spider?<br> 在main.py中我们通过代码的方式启动爬虫，因此可以设置你期望的爬虫类。</li><li>如何定制spider的middleware和pipeline?<br> 这个属于scrapy的基本功能，可以在爬虫类中设置custom_settings属性。</li><li>动态网页如何爬取?<br> 该项目中引入了selenium依赖，需要你在middleware中对请求和响应通过selenium进行处理。可以参考ChromeDownloaderMiddleware</li><li>为什么redis中没有数据?<br> 首先查看日志中是否有报错日志。如果没有，需要检查xpath是否正确。在页面上f12，通过$x即可测试。</li><li>为什么爬虫直接退出了，且没有报错?<br> 如果是从redis拉取数据进行爬取，查看是否继承了RedisSpider或RedisCategorySpider。RedisSpider需要指定redis_key参数，RedisCategorySpider提供了一个从公共枚举和配置中获取该参数的方法，因此需要你指定枚举UrlType以及在配置文件（spider_config）中设置参数配置，该参数应该和之前的链接提取spider参数相同。</li><li>为什么浏览器一直白屏，后台不报错，似乎爬虫停住了?<br> 检查下redis中是否有数据，否则爬虫会等待redis中有值后才会继续。</li></ol><h2 id="获取" tabindex="-1"><a class="header-anchor" href="#获取"><span>获取</span></a></h2><p>项目源码地址：<a href="https://gitee.com/NikolaZhang/look_look" target="_blank" rel="noopener noreferrer">https://gitee.com/NikolaZhang/look_look</a></p>`,50)])])}const p=e(n,[["render",r]]),h=JSON.parse('{"path":"/open_source/python-tools/look_look.html","title":"look_look介绍","lang":"en-US","frontmatter":{"date":"2023-05-09T00:00:00.000Z","title":"look_look介绍","shortTitle":"有什么好东西，让我康康！","description":"这是一个基于scrapy框架的爬虫应用，提供爬虫的通用功能。","tag":["mybatis","vscode","插件"],"category":["开源"],"banner":"http://image.nikolazhang.top/wallhaven-nrwq11.jpg","author":"nikola","icon":"article","isOriginal":true,"sticky":false,"timeline":true,"article":true,"star":true,"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"look_look介绍\\",\\"image\\":[\\"https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/url.png\\",\\"https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/article.png\\",\\"https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/article_content.png\\",\\"https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work1.png\\",\\"https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work2.png\\",\\"https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work3.png\\"],\\"datePublished\\":\\"2023-05-09T00:00:00.000Z\\",\\"dateModified\\":\\"2024-12-31T08:40:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"nikola\\"}]}"],["meta",{"property":"og:url","content":"https://nikolazhang.github.io/open_source/python-tools/look_look.html"}],["meta",{"property":"og:title","content":"look_look介绍"}],["meta",{"property":"og:description","content":"这是一个基于scrapy框架的爬虫应用，提供爬虫的通用功能。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"http://image.nikolazhang.top/wallhaven-nrwq11.jpg"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-12-31T08:40:08.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"http://image.nikolazhang.top/wallhaven-nrwq11.jpg"}],["meta",{"name":"twitter:image:alt","content":"look_look介绍"}],["meta",{"property":"article:author","content":"nikola"}],["meta",{"property":"article:tag","content":"插件"}],["meta",{"property":"article:tag","content":"vscode"}],["meta",{"property":"article:tag","content":"mybatis"}],["meta",{"property":"article:published_time","content":"2023-05-09T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-12-31T08:40:08.000Z"}]]},"git":{"createdTime":1683796175000,"updatedTime":1735634408000,"contributors":[{"name":"dewy yr","username":"","email":"nikolazhang@163.com","commits":2},{"name":"我小叮当","username":"","email":"nikolazhang@163.com","commits":5}]},"readingTime":{"minutes":5.6,"words":1679},"filePathRelative":"open_source/python-tools/look_look.md"}');export{p as comp,h as data};
